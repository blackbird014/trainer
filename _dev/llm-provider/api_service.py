"""
FastAPI service for LLM Provider microservice.

Exposes LLM provider functionality via REST API.
Allows swapping providers via configuration.
Run on port 8001 (configurable).
"""

import os
import time
from fastapi import FastAPI, HTTPException
from fastapi.responses import PlainTextResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Dict, Any, Optional, List
import logging

try:
    from prometheus_client import generate_latest, CONTENT_TYPE_LATEST, Counter, Histogram
    PROMETHEUS_AVAILABLE = True
    
    # Prometheus metrics
    llm_requests_total = Counter(
        'llm_provider_requests_total',
        'Total number of LLM requests',
        ['provider', 'model', 'status']
    )
    llm_request_duration = Histogram(
        'llm_provider_request_duration_seconds',
        'LLM request duration in seconds',
        ['provider', 'model']
    )
    llm_tokens_total = Counter(
        'llm_provider_tokens_total',
        'Total tokens used',
        ['provider', 'model', 'type']  # type: prompt or completion
    )
    llm_cost_total = Counter(
        'llm_provider_cost_total',
        'Total cost in USD',
        ['provider', 'model']
    )
except ImportError:
    PROMETHEUS_AVAILABLE = False
    print("⚠ Prometheus client not available")

from llm_provider import (
    create_provider,
    get_registry,
    CompletionResult,
    LLMProvider
)

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize FastAPI app
app = FastAPI(
    title="LLM Provider API",
    description="Microservice for LLM provider operations with provider swapping",
    version="0.1.0"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Provider configuration from environment
DEFAULT_PROVIDER = os.getenv("LLM_PROVIDER", "mock")
DEFAULT_MODEL = os.getenv("LLM_MODEL", "mock-model")
USE_MOCK = os.getenv("LLM_USE_MOCK", "true").lower() == "true"

# Create a mock provider class for v1
class MockProvider(LLMProvider):
    """Mock LLM provider for testing and development."""
    
    def complete(self, prompt: str, **kwargs) -> CompletionResult:
        """Generate a mock response."""
        # Generate deterministic mock response
        ticker = kwargs.get('ticker', '')
        content = f"""# Stock Analysis Report

This is a mock analysis generated for demonstration purposes.

## Company Overview
- Ticker: {ticker if ticker else 'UNKNOWN'}
- Analysis Date: {time.strftime('%Y-%m-%d %H:%M:%S')}

## Key Metrics
- Market analysis: Based on current market conditions
- Financial health: Assessment pending real data
- Growth prospects: Requires further analysis

## Recommendations
- This is a mock response for testing purposes
- Real analysis would be generated by an actual LLM provider
- Switch to a real provider (OpenAI, Anthropic, Bedrock) for production use

*Note: This response was generated by the MockProvider for demonstration.*"""
        
        # Estimate tokens (rough approximation)
        tokens_used = len(prompt.split()) + len(content.split()) + 50
        
        return CompletionResult(
            content=content,
            tokens_used=tokens_used,
            model="mock",
            provider="mock",
            cost=0.0
        )
    
    def stream(self, prompt: str, **kwargs):
        """Stream mock response."""
        content = self.complete(prompt, **kwargs).content
        # Split into chunks for streaming
        words = content.split()
        chunk_size = max(1, len(words) // 10)
        for i in range(0, len(words), chunk_size):
            yield " ".join(words[i:i+chunk_size]) + " "
    
    def get_cost(self, tokens: int) -> float:
        """Mock cost calculation."""
        return 0.0

# Register mock provider if not already registered
registry = get_registry()
if "mock" not in registry.list_providers():
    registry.register("mock", MockProvider, is_default=USE_MOCK)
    logger.info("✓ Mock provider registered")

# Initialize default provider
_current_provider: Optional[LLMProvider] = None
_current_provider_name: str = DEFAULT_PROVIDER
_current_model: str = DEFAULT_MODEL


def get_provider(provider_name: Optional[str] = None, model: Optional[str] = None, use_mock: Optional[bool] = None) -> LLMProvider:
    """
    Get or create a provider instance.
    
    Args:
        provider_name: Provider name (e.g., "openai", "mock"). If None, uses default.
        model: Model name. If None, uses default.
        use_mock: Force mock mode. If None, uses environment variable.
    
    Returns:
        LLMProvider instance
    """
    global _current_provider, _current_provider_name, _current_model
    
    # Determine provider
    if use_mock is True or (use_mock is None and USE_MOCK):
        provider_name = "mock"
        model = "mock-model"
    elif provider_name is None:
        provider_name = DEFAULT_PROVIDER
    if model is None:
        model = DEFAULT_MODEL
    
    # Reuse existing provider if same config
    if (_current_provider is not None and 
        _current_provider_name == provider_name and 
        _current_model == model):
        return _current_provider
    
    # Create new provider
    try:
        if provider_name == "mock":
            _current_provider = MockProvider("mock", "mock-model")
        else:
            _current_provider = create_provider(provider_name, model=model)
        
        _current_provider_name = provider_name
        _current_model = model
        
        logger.info(f"✓ Provider initialized: {provider_name} / {model}")
        return _current_provider
    except Exception as e:
        logger.error(f"Failed to create provider {provider_name}: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to create provider '{provider_name}': {str(e)}"
        )


# Request/Response models
class GenerateRequest(BaseModel):
    prompt: str
    provider: Optional[str] = None
    model: Optional[str] = None
    use_mock: Optional[bool] = None
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    ticker: Optional[str] = None  # For stock analysis context
    extra_kwargs: Optional[Dict[str, Any]] = None  # Additional provider-specific parameters


class GenerateResponse(BaseModel):
    success: bool
    content: str
    provider: str
    model: str
    tokens_used: int
    cost: float
    metadata: Optional[Dict[str, Any]] = None
    error: Optional[str] = None


class ListProvidersResponse(BaseModel):
    providers: List[str]
    default_provider: str
    current_provider: str
    current_model: str


# Routes
@app.get("/")
async def root():
    """API information."""
    return {
        "name": "LLM Provider API",
        "version": "0.1.0",
        "endpoints": {
            "generate": "POST /generate",
            "stream": "POST /stream",
            "list_providers": "GET /providers",
            "metrics": "GET /metrics",
            "health": "GET /health",
        },
        "configuration": {
            "default_provider": DEFAULT_PROVIDER,
            "default_model": DEFAULT_MODEL,
            "use_mock": USE_MOCK,
            "current_provider": _current_provider_name,
            "current_model": _current_model
        }
    }


@app.get("/health")
async def health():
    """Health check endpoint."""
    try:
        provider = get_provider()
        return {
            "status": "healthy",
            "provider": _current_provider_name,
            "model": _current_model
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e)
        }


@app.get("/providers", response_model=ListProvidersResponse)
async def list_providers():
    """List all available providers."""
    registry = get_registry()
    providers = registry.list_providers()
    default = registry.get_default() or DEFAULT_PROVIDER
    
    return ListProvidersResponse(
        providers=providers,
        default_provider=default,
        current_provider=_current_provider_name,
        current_model=_current_model
    )


@app.post("/generate", response_model=GenerateResponse)
async def generate(request: GenerateRequest):
    """
    Generate a completion from an LLM provider.
    
    Supports provider swapping via request parameters:
    - provider: Provider name (e.g., "openai", "anthropic", "mock")
    - model: Model name
    - use_mock: Force mock mode (overrides provider)
    - ticker: Optional ticker for context (used by mock provider)
    """
    start_time = time.time()
    
    try:
        # Get provider (with potential swap)
        provider = get_provider(
            provider_name=request.provider,
            model=request.model,
            use_mock=request.use_mock
        )
        
        # Prepare kwargs for provider.complete()
        kwargs = {}
        if request.temperature is not None:
            kwargs["temperature"] = request.temperature
        if request.max_tokens is not None:
            kwargs["max_tokens"] = request.max_tokens
        if request.ticker:
            kwargs["ticker"] = request.ticker
        # Add any additional kwargs from request
        if request.extra_kwargs:
            kwargs.update(request.extra_kwargs)
        
        # Generate completion
        result: CompletionResult = provider.complete(request.prompt, **kwargs)
        
        # Calculate duration
        duration = time.time() - start_time
        
        # Update Prometheus metrics
        if PROMETHEUS_AVAILABLE:
            llm_requests_total.labels(
                provider=result.provider,
                model=result.model,
                status="success"
            ).inc()
            llm_request_duration.labels(
                provider=result.provider,
                model=result.model
            ).observe(duration)
            llm_tokens_total.labels(
                provider=result.provider,
                model=result.model,
                type="completion"
            ).inc(result.tokens_used)
            llm_cost_total.labels(
                provider=result.provider,
                model=result.model
            ).inc(result.cost)
        
        return GenerateResponse(
            success=True,
            content=result.content,
            provider=result.provider,
            model=result.model,
            tokens_used=result.tokens_used,
            cost=result.cost,
            metadata=result.metadata if hasattr(result, 'metadata') else None
        )
        
    except HTTPException:
        raise
    except Exception as e:
        duration = time.time() - start_time
        
        # Update error metrics
        if PROMETHEUS_AVAILABLE:
            provider_name = request.provider or _current_provider_name
            model_name = request.model or _current_model
            llm_requests_total.labels(
                provider=provider_name,
                model=model_name,
                status="error"
            ).inc()
        
        logger.error(f"Error generating completion: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Failed to generate completion: {str(e)}"
        )


@app.post("/stream")
async def stream(request: GenerateRequest):
    """
    Stream a completion from an LLM provider.
    
    Returns a streaming response with chunks of the completion.
    """
    try:
        # Get provider
        provider = get_provider(
            provider_name=request.provider,
            model=request.model,
            use_mock=request.use_mock
        )
        
        # Prepare kwargs
        kwargs = {}
        if request.temperature is not None:
            kwargs["temperature"] = request.temperature
        if request.max_tokens is not None:
            kwargs["max_tokens"] = request.max_tokens
        if request.ticker:
            kwargs["ticker"] = request.ticker
        if request.extra_kwargs:
            kwargs.update(request.extra_kwargs)
        
        # Stream completion
        def generate():
            for chunk in provider.stream(request.prompt, **kwargs):
                yield chunk
        
        return StreamingResponse(generate(), media_type="text/plain")
        
    except Exception as e:
        logger.error(f"Error streaming completion: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Failed to stream completion: {str(e)}"
        )


@app.get("/metrics", response_class=PlainTextResponse)
async def metrics():
    """Prometheus metrics endpoint."""
    if not PROMETHEUS_AVAILABLE:
        return PlainTextResponse(
            "# Prometheus client not available\n", 503
        )
    
    return generate_latest()


if __name__ == "__main__":
    import uvicorn
    
    port = int(os.getenv("PORT", 8001))
    
    print("=" * 80)
    print("LLM Provider FastAPI Service")
    print("=" * 80)
    print()
    print(f"Starting server on http://0.0.0.0:{port}")
    print()
    print("Configuration:")
    print(f"  - Default Provider: {DEFAULT_PROVIDER}")
    print(f"  - Default Model: {DEFAULT_MODEL}")
    print(f"  - Use Mock: {USE_MOCK}")
    print()
    print("Endpoints:")
    print(f"  - http://localhost:{port}/              - API info")
    print(f"  - http://localhost:{port}/docs          - Swagger UI")
    print(f"  - http://localhost:{port}/metrics       - Prometheus metrics")
    print(f"  - http://localhost:{port}/health         - Health check")
    print(f"  - http://localhost:{port}/providers     - List providers")
    print(f"  - http://localhost:{port}/generate      - Generate completion")
    print(f"  - http://localhost:{port}/stream        - Stream completion")
    print()
    print("Available Providers:")
    registry = get_registry()
    for provider_name in registry.list_providers():
        print(f"  - {provider_name}")
    print()
    if PROMETHEUS_AVAILABLE:
        print("Metrics:")
        print("  - llm_provider_requests_total")
        print("  - llm_provider_request_duration_seconds")
        print("  - llm_provider_tokens_total")
        print("  - llm_provider_cost_total")
        print()
    print("=" * 80)
    print()
    
    uvicorn.run(app, host="0.0.0.0", port=port)

